#!/usr/bin/env python

from __future__ import print_function
import icedbProgSetup  # noqa: F401
import sys
import argparse
from gencode_icedb.rsl.rslModel import sqliteConnect, RunMetadata, MappingMetadata, MappingParameters, SpliceJuncSupport
from gencode_icedb.rsl.rslModelCache import MappingParametersCache, PutativeIntronCache
from gencode_icedb.rsl.starResultsDir import StarMappingParameters, StarResultsDir
from gencode_icedb.rsl import starOps
from pycbio.sys import loggingOps
from pycbio.tsv import TabFile

bulk_size = 100   # size of each bulk insert


def parseArgs():
    desc = """Load introns support from STAR splice junctions into the database.
    """
    parser = argparse.ArgumentParser(description=desc)
    loggingOps.addCmdOptions(parser)
    parser.add_argument('--numprocs', type=int, default=4,
                        help="""number of process uses to load database in parallel.""")
    parser.add_argument('rsldb',
                        help="""sqllite3 database, tables are created as needed""")
    parser.add_argument("mappingParamsTsv",
                        help="""mapping parameters TSV""")
    parser.add_argument('starResultsDirTsv',
                        help="""TSV file with column `sjout', which has the path to the splice junction file, which maybe compressed."""
                        """The path is relative to the directory containing starResultsTsv""")
    opts = parser.parse_args()
    loggingOps.setupFromCmd(opts, sys.argv[0])
    return opts


def dbLoadMappingParameters(dbconn, mappingParamsTsv):
    """load mapping parameters into database if they don't already exist,
    return cache of MappingParameters objects."""
    MappingParameters.create_table(fail_silently=True)
    mappingParamsCache = MappingParametersCache()
    with dbconn.atomic():
        StarMappingParameters(mappingParamsTsv).updateDatabase(mappingParamsCache)
    return mappingParamsCache


class SjSupportLoader(object):
    """Load sjout files from one STAR run."""
    # tried running this is with multiprocessing, but it was not faster
    def __init__(self, dbconn, mappingParamsCache, pintronCache):
        self.dbconn = dbconn
        self.mappingParamsCache = mappingParamsCache
        self.pintronCache = pintronCache

    def __createMappingMetadata(self, starResult, mappingParams):
        runMetadata = RunMetadata.get(run_acc=starResult.run_acc)
        mappingMetadata = MappingMetadata(run_metadata_id=runMetadata.id,
                                          mapping_symid=starResult.mapping_symid,
                                          mapping_parameters_id=mappingParams.id)
        mappingMetadata.save()
        return mappingMetadata

    def __makeSjSupportDict(self, sjOutRow, mappingMetadata):
        "construct dict for bulk insert"
        pintron = self.pintronCache.fetchByLoc(sjOutRow.chrom, sjOutRow.start - 1, sjOutRow.end,
                                               starOps.starStrandCodeToChar(sjOutRow.strand))
        return {"mapping_metadata_id": mappingMetadata.id,
                "putative_intron_id": pintron.id,
                "annotated": sjOutRow.annotated,
                "num_uniq_reads": sjOutRow.num_uniq_reads,
                "num_multi_reads": sjOutRow.num_multi_reads,
                "max_overhang": sjOutRow.max_overhang}

    def __makeSjSupportDicts(self, sjOutFile, mappingMetadata):
        "build dict for all rows for bulk insert"
        return [self.__makeSjSupportDict(sjOutRow, mappingMetadata)
                for sjOutRow in TabFile(sjOutFile, rowClass=starOps.StarSjOutRow.factory)]

    def __dbLoad(self, recs):
        with self.dbconn.atomic():
            for idx in xrange(0, len(recs), bulk_size):
                SpliceJuncSupport.insert_many(recs[idx:idx + bulk_size]).execute()

    def load(self, starResult):
        """Load sjout files from one STAR run.  This can run in another
        multiprocessing process"""
        mappingParams = self.mappingParamsCache.fetchBySymId(starResult.mapping_param_symid)
        mappingMetadata = self.__createMappingMetadata(starResult, mappingParams)
        sjSupportsDicts = self.__makeSjSupportDicts(starResult.sjoutPath, mappingMetadata)
        self.__dbLoad(sjSupportsDicts)


def rslStarSjSupportDbLoad(opts):
    dbconn = sqliteConnect(opts.rsldb, synchronous=False)
    mappingParamsCache = dbLoadMappingParameters(dbconn, opts.mappingParamsTsv)
    starResultsDir = StarResultsDir(opts.starResultsDirTsv)
    pintronCache = PutativeIntronCache(dbconn)  # load all into memory
    MappingMetadata.create_table(fail_silently=True)
    SpliceJuncSupport.create_table(fail_silently=True)

    loader = SjSupportLoader(dbconn, mappingParamsCache, pintronCache)

    for starResult in starResultsDir:
        loader.load(starResult)


rslStarSjSupportDbLoad(parseArgs())
