#!/usr/bin/env python3
import icedbProgSetup  # noqa: F401
import os
import argparse
from pycbio.sys import fileOps
from pycbio.tsv import TsvReader, tsvRowToDict
from gencode_icedb.tsl.supportDefs import TrascriptionSupportLevel, EvidenceSupport
from gencode_icedb.tsl.tslModels import tslConnect, tslClose, GencodeTranscriptSupport, GencodeTranscriptSupportDetails
from gencode_icedb.tsl.evidenceDb import EvidenceSource

bulk_size = 100   # size of each bulk insert

# FIXME: move fast bulk load stuff to sqliteOps. in pycbio, generally, this will
# still way too much to save with all details

setPragmaSql = (
    "PRAGMA cache_size = 1000000;"
    "PRAGMA synchronous = OFF;"
    "PRAGMA journal_mode = OFF;"
    "PRAGMA locking_mode = EXCLUSIVE;"
    "PRAGMA count_changes = OFF;"
    "PRAGMA temp_store = MEMORY;"
    "PRAGMA auto_vacuum = NONE;")


def parseArgs():
    desc = """Combine tslCollectGenbankSupport job results and store in an SQLite3 table."""
    parser = argparse.ArgumentParser(description=desc)
    parser.add_argument('--saveDetails', action='store_true', default=False,
                        help="""Save the details in a table.  This is very slow if full details have been saved.""")
    parser.add_argument('workDir',
                        help="""directory contains results directory""")
    parser.add_argument('resultsDb',
                        help="""Sqlite database were results are stored."""
                        """This does not append results, existing gencodesupport table is dropped.""")
    return parser.parse_args()


def getCreateIndexes(cursor, tbl):
    sql = "select sql from sqlite_master WHERE tbl_name = '{tbl}';".format(tbl=tbl)
    cursor.execute(sql)
    return [r[0] for r in cursor if r[0].startswith("CREATE INDEX")]


def createsToDrops(creates):
    return ["DROP INDEX {};".format(c.split(' ')[2]) for c in creates]


def dropIndexes(cursor, creates):
    "drop indexes to speed up batch load"
    drops = createsToDrops(creates)
    cursor.execute(" ".join(drops))


def preBulkInsert(conn, tbl):
    cursor = conn.cursor()
    creates = getCreateIndexes(cursor, tbl)
    dropIndexes(cursor, creates)
    cursor.execute(setPragmaSql)
    for row in cursor:
        pass  # discard results
    cursor.close()
    return creates


def postBulkInsert(conn, creates):
    cursor = conn.cursor()
    for create in creates:
        cursor.execute(create)
    cursor.close()


resultsTsvTypeMap = {
    "level": TrascriptionSupportLevel
}


def bulkInsert(conn, tblcls, recs):
    creates = preBulkInsert(conn, tblcls.get_table_name())
    with conn.atomic():
        for idx in range(0, len(recs), bulk_size):
            tblcls.insert_many(recs[idx:idx + bulk_size]).execute()
    postBulkInsert(conn, creates)


def convertRowToDict(row):
    rec = tsvRowToDict(row)
    rec["intLevel"] = row.level.value
    return rec


def readResultsTsv(resultsTsv):
    return [convertRowToDict(row) for row in TsvReader(resultsTsv, typeMap=resultsTsvTypeMap)]


def dbInsertResults(conn, expectedTsvs):
    GencodeTranscriptSupport.create_table()
    recs = []
    for resultsTsv in expectedTsvs:
        recs.extend(readResultsTsv(resultsTsv))
    bulkInsert(conn, GencodeTranscriptSupport, recs)


resultDetailsTsvTypeMap = {
    "evidSrc": EvidenceSource,
    "evidSupport": EvidenceSupport,
}


def readDetailsTsv(resultDetailsTsv):
    return [tsvRowToDict(row) for row in TsvReader(resultDetailsTsv, typeMap=resultDetailsTsvTypeMap)]


def resultsTsvToDetailsPath(resultsTsv):
    " *.tsl.tsv -> *.details.tsv"
    # FIXME change to save both paths in expected as with utrAddCollect
    p1 = os.path.splitext(os.path.splitext(resultsTsv)[0])[0]
    return "{}.details.tsv".format(p1)


def dbInsertDetails(conn, expectedTsvs):
    GencodeTranscriptSupportDetails.create_table()
    recs = []
    for resultsTsv in expectedTsvs:
        recs.extend(readDetailsTsv(resultsTsvToDetailsPath(resultsTsv)))
    bulkInsert(conn, GencodeTranscriptSupportDetails, recs)


def tslCollectGenbankSupportFinishJobs(opts):
    "main function"
    fileOps.ensureFileDir(opts.resultsDb)
    conn = tslConnect(opts.resultsDb, create=True, readonly=False)
    GencodeTranscriptSupport.drop_table(fail_silently=True)
    GencodeTranscriptSupportDetails.drop_table(fail_silently=True)
    expectedTsvs = fileOps.readFileLines(os.path.join(opts.workDir, "expected.lst"))
    dbInsertResults(conn, expectedTsvs)
    if opts.saveDetails:
        dbInsertDetails(conn, expectedTsvs)
    tslClose(conn)


tslCollectGenbankSupportFinishJobs(parseArgs())
